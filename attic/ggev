#!/usr/bin/env python3
"""Google things"""
# pyright: reportMissingImports=false
# mypy: disable-error-code="import-untyped"

from collections import defaultdict
import hashlib
import json
import re
import subprocess
import urllib.parse

from bidi.algorithm import get_display

CONFIG = __file__ + ".json"
HISTORY = __file__ + ".history"

with open(CONFIG, encoding="utf-8") as _fo:
    config = json.load(_fo)

BLACKLIST_PREFIXES = set(config["BLACKLIST_PREFIXES"])

PURPLE = "\033[95m"
CYAN = "\033[96m"
DARKCYAN = "\033[36m"
BLUE = "\033[94m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
BOLD = "\033[1m"
UNDERLINE = "\033[4m"
END = "\033[0m"


class SearchError(Exception):
    """Search engine says NO"""


def main():
    """Google things"""
    try:
        hits = {}
        for kwargs in config["SEARCHES"]:
            # hits.update(google(**kwargs))
            hits.update(ddg(**kwargs))
    except SearchError as ge:
        print(ge)
        return

    for hit in hits.values():
        abstract = hit["abstract"]
        abstract = re.sub(
            r"^\s*—\s*(לפני |\d+ ב).*$",
            r"",
            abstract,
        )
        hit["md5"] = md5sum(abstract)

    history = defaultdict(set)
    try:
        with open(HISTORY, "r", encoding="utf-8") as fo:
            for line in fo:
                url, *md5s = line.strip().split("\t")
                history[url] = set(md5s)
    except FileNotFoundError:
        pass

    added = []
    for url, hit in hits.items():
        if hit["md5"] not in history[url]:
            added.append(url)
            history[url].add(hit["md5"])

    if not added:
        print("NO NEWS.")
    else:
        for n, url in enumerate(sorted(added), 1):
            hit = hits[url]
            title = get_display(hit["title"])
            print(f"{n}. {BOLD}{title}{END}")
            print(urllib.parse.unquote(url))
            print("")
            print(get_display(hit["abstract"]))
            print("")

        with open(HISTORY, "w", encoding="utf-8") as fo:
            for url, md5s in history.items():
                fo.write("\t".join([url] + list(md5s)))
                fo.write("\n")


GG2DDG_LANG = {
    "iv": "il-he",
}

GG2DDG_TIME = {
    "m1": "m",
    "w1": "w",
}


def ddg(query, lang=None, timeframe="m1"):
    """Call DuckDuckGo"""
    cmd = ["ddgr", "--json"]
    if lang is not None:
        cmd.append("--reg")
        cmd.append(GG2DDG_LANG[lang])
    if timeframe:
        cmd.append("--time")
        cmd.append(GG2DDG_TIME[timeframe])
    cmd.append(query)
    return call_searcher(cmd)


def google(query, lang=None, timeframe="m1"):
    """DEPRECATED"""
    cmd = ["googler"]
    if lang:
        cmd.extend(["-l", lang])
    cmd.extend(["--json", "-t", timeframe, query])
    return call_searcher(cmd)


def call_searcher(cmd):
    """Call any kind of googler-like searcher app"""
    out = subprocess.run(cmd, capture_output=True, check=False)
    if out.returncode != 0:
        raise SearchError(out.stderr.decode())
    return {
        hit["url"]: hit
        for hit in json.loads(out.stdout)
        if not blacklisted(hit["url"])
    }


def blacklisted(url):
    """Is the URL blacklisted?"""
    return any(url.startswith(prefix) for prefix in BLACKLIST_PREFIXES)


def md5sum(text):
    """Calculate MD5"""
    return hashlib.md5(text.encode("utf-8")).hexdigest()


if __name__ == "__main__":
    main()
